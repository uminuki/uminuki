# -*- coding: utf-8 -*-
"""Exercise-1p2-Dizon

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WXxa_mGmWCy8IhTuHgCnG9MM-36x7XUW

# Exercise 1 Part 2

Exercise 1 Part 2 includes a **programming assignment with 4 problems** (9 points) and a **feedback/workload assessment assignment** (1 point). For each problem you need to modify the notebook by adding your own programming solutions or written text.

### Due date

You should submit your Exercise link to our UVLe Submission folder **two weeks after the first practical session (submit by Wednesday November 5)**

**A couple of hints**:

- You can **execute a cell** by clicking a given cell that you want to run and pressing <kbd>Shift</kbd> + <kbd>Enter</kbd> (or by clicking the "Play" button on top)
- You can **change the cell-type** between `Markdown` (for writing text) and `Code` (for writing/executing code) from the dropdown menu above.

See [**further details and help from here**](https://pythongis.org/part1/chapter-01/nb/04-using-jupyterlab.html).

<hr style="border:2px solid gray">

# Part 1: Programming assignment (9 points)

In this exercise, we practice doing spatial regression using TripAdvisor data from Metro Manila that is generated using a public scraper. We aim to understand what factors influence the vacation rental prices in Metro Manila. In this exercise, you will learn how to:

 - investigate linear relationship between different attributes
 - create spatial weights using pysal library
 - investigate spatial autocorrelation in the data using Moran's I indicator
 - conduct Ordinary Least Squares regression and spatial regression models using pysal library


### Due date

The exercise should be returned by the end of Wednesday (5th of November, 2024) *Remember, remember the 5th of November*.

<hr style="border:2px solid gray">

## Input data

In this Exercise, we will use TripAdvisor rental rates which was scraped using [TripAdvisor Scraper](https://apify.com/maxcopell/tripadvisor). We will mainly utilize two datasets
  1. TripAdvisor Vacation Rental Dataset - Provides us rental rates of vacation homes within Metro Manila. Additional information on the attributes of the rental place is also available.
  2. OpenStreetMap data - Provides us street network data that we use to add proximity variables to take into account Spatial Heterogeneity.

## Hypothesis

In this exercise, we hypothesize that following attributes might explain the average vacation rental price:

 1. Number of Bathrooms
 2. Number of Beds
 3. Choice of 2 amenities

Further, to add spatial context in the model and addressing spatial heterogeneity, we add two proximity variables,

 4. Distance to nearest Restaurant
 5. Distance to nearest major road.

Finally, we consider using a spatial lag model that:

 6. The average price of the neighboring areas influences the price on a given area

## Problem 0 - Download data

Before starting the exercise, download the necessary data by accessing the following [link](https://drive.google.com/file/d/14RcT1l9skdIRgQDv0luRKbyDLLUlQuM2/view?usp=sharing). Make sure to store it in your Google Drive for easier access

Observing the dataset, one of its columns is the amenity data. Here's a list of possible values within the amenity list.

{'1 bathroom(s)',
 '1 bedroom(s)',
 '1 bunk bed(s)',
 '1 cot(s)',
 '1 double bed(s)',
 '1 en-suite room(s)',
 '1 full bath(s)',
 '1 half bath(s)',
 '1 king bed(s)',
 '1 queen bed(s)',
 '1 shower(s)',
 '1 single bed(s)',
 '1 sofa bed(s)',
 '10 single bed(s)',
 '2 bathroom(s)',
 '2 bedroom(s)',
 '2 bunk bed(s)',
 '2 cot(s)',
 '2 double bed(s)',
 '2 en-suite room(s)',
 '2 full bath(s)',
 '2 queen bed(s)',
 '2 shower(s)',
 '2 single bed(s)',
 '2 sofa bed(s)',
 '3 bathroom(s)',
 '3 bedroom(s)',
 '3 bunk bed(s)',
 '3 en-suite room(s)',
 '3 full bath(s)',
 '3 king bed(s)',
 '3 single bed(s)',
 '4 bathroom(s)',
 '4 bedroom(s)',
 '4 full bath(s)',
 '4 queen bed(s)',
 '4 single bed(s)',
 '5 bathroom(s)',
 '50 bedroom(s)',
 '6 single bed(s)',
 'Air conditioning',
 'Balcony',
 'Beach access',
 'Beachfront/Lakeside',
 'Cable television',
 'Ceiling Fans',
 'Central heating',
 "Children's pool",
 'City getaway',
 'Corporate bookings allowed',
 'Countryside area',
 'Cycling areas',
 'DVD player',
 'Deck',
 'Dining table seats: 1',
 'Dining table seats: 12',
 'Dining table seats: 2',
 'Dining table seats: 3',
 'Dining table seats: 4',
 'Dining table seats: 5',
 'Dining table seats: 6',
 'Dining table seats: 8',
 'Dishwasher',
 'Dryer',
 'Elder access: ASK',
 'Elder access: NO',
 'Elder access: YES',
 'Elevator/Lift access',
 'Fireplace',
 'Fishing spots available',
 'Fits 10 pax',
 'Fits 12 pax',
 'Fits 13 pax',
 'Fits 14 pax',
 'Fits 15 pax',
 'Fits 16 pax',
 'Fits 2 pax',
 'Fits 20 pax',
 'Fits 3 pax',
 'Fits 4 pax',
 'Fits 5 pax',
 'Fits 6 pax',
 'Fits 7 pax',
 'Fits 8 pax',
 'Freezer',
 'Game Room',
 'Garage',
 'Golf course (within 15 min walk)',
 'Golf course (within 30 mins drive)',
 'Grill',
 'Gym',
 'Hairdryer',
 'Highchair',
 'Hiking areas',
 'Hot tub',
 'Housekeeping included',
 'Housekeeping optional',
 'Internet access',
 'Iron',
 'Kettle',
 'Kid friendly',
 'Kid friendly: ASK',
 'Kid friendly: NO',
 'Kid friendly: YES',
 'Linens provided',
 'Living room seats: 1',
 'Living room seats: 10',
 'Living room seats: 12',
 'Living room seats: 2',
 'Living room seats: 3',
 'Living room seats: 4',
 'Living room seats: 5',
 'Living room seats: 6',
 'Living room seats: 8',
 'Long-term rental allowed (>1 month)',
 'Microwave',
 'Mountain views',
 'Nightlife area',
 'Ocean views',
 'Outdoor Dining Area',
 'Paper towels provided',
 'Parking available',
 'Parties allowed',
 'Patio',
 'Pet friendly: ASK',
 'Pet friendly: NO',
 'Pet friendly: YES',
 'Ping Pong table',
 'Playground',
 'Pool table',
 'Private indoor pool',
 'Private outdoor pool (unheated)',
 'Private yard',
 'Refrigerator',
 'Safe',
 'Sauna',
 'Secure parking available',
 'Shared indoor pool',
 'Shared outdoor pool (heated)',
 'Shared outdoor pool (unheated)',
 'Shared tennis court',
 'Shared yard',
 'Short-term rental only (<5 days)',
 'Smoking allowed: ASK',
 'Smoking allowed: NO',
 'Smoking allowed: YES',
 'Soap and shampoo provided',
 'Staff on property',
 'Stereo system',
 'Stove',
 'Swingset',
 'Telephone',
 'Television',
 'Tennis court',
 'Terrace',
 'Toaster',
 'Towels provided',
 'Trampoline',
 'Washer',
 'Waterfront location',
 'Waterfront views',
 'Watersports',
 'Wheelchair access: ASK',
 'Wheelchair access: NO',
 'Wheelchair access: YES',
 'Wi-fi'}

## Problem 1 - Prepare data (3 points)

In this problem you should:

- Read the tripadvisor data and generate a dataframe
- You may remove unnecessary columns. Retain the columns amenities, bathcount, baseDailyRate, and bedroomInfo.
- Transform the tripadvisor dataframe into a geodataframe using the columns latitude and longitude
- Get the dataset that is within the boundaries of Metro Manila from OSM. You may want to recall your Exercise 1 or Tech Session 1 files for retrieving the OSM dataset using OSMnx.
- One important column is the pricing information which can be found in the baseDailyRate column. Observing the baseDailyRate column and its general pattern, perform a data cleaning algorithm (similar to our tech session) transforming the contents of the baseDailyRate data to a numeric type.
- Transform the bedroomInfo column to extract numeric characters in the string value of the column. You may use [regex](https://stackoverflow.com/questions/4289331/how-to-extract-numbers-from-a-string-in-python) to extract numbers from the string value. This will return a list of numbers that are within your string. Afterwards, find the sum of the numbers in the said list. This will represent the number of beds of the rental unit.
- From the list of amenities, choose two of the possible amenities that you wish to include as your explanatory variable. You may want to follow the "pool" amenity example in our technical session, wherein we simply create a new column signifying the presence or absence of a pool
- Load the road network and the restaurant data in Metro Manila. Filter the data such that it restaurants are points only and road network are lines only
- For all features within the tripadvisor geodataframe, calculate the shortest distance between the point and the (a) nearest restaurant and (b) nearest primary road. You may want to us [sjoin_nearest](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.sjoin_nearest.html). Make sure to store the distance value by specifying the column value for distance_col argument.
- Investigate your data. Are there missing values in any of the selected attributes? If there are:

   - remove NaN values
   - reset the index

- Make map out of the prices of the rental places. You may choose any style that is visually appealing for you.

<br>

Please write your solution to the cell below (remove the `raise NotImplementedError()` code). You can create new cells as well if needed.
"""

# Install needed packages
!pip install mapclassify
!pip install osmnx
!pip install pysal

import geopandas as gpd
import pandas as pd
import numpy as np
import osmnx as ox
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

# Mount Google Drive for data retrival
from google.colab import drive
drive.mount("/content/drive")

# Read tripadvisor data

# Filepath to tripadvisor data
fp = "/content/drive/MyDrive/Personal Acad Materials/AY 24-25/GE 197/LabEx/LabEx 1/tripadvisor_gme222.json"

# Read tripadvisor data then convert into a dataframe
# Retain only name, amenities, bathcount, baseDailyRate, and bedroominfo columns
tripadvisor_data = pd.read_json(fp)
tripadvisor_df = tripadvisor_data[["name", "amenities", "bathCount", "baseDailyRate", "bedroomInfo"]]
#tripadvisor_df.head()

# Transform tripadvisor dataframe into a geodataframe
tripadvisor_df["geometry"] = gpd.points_from_xy(tripadvisor_data["longitude"], tripadvisor_data["latitude"])
tripadvisor_gdf = gpd.GeoDataFrame(tripadvisor_df, crs="epsg:4326")
#tripadvisor_gdf.head()

# Download and extract OSM data for Metro Manila
mm_boundary = ox.geocode_to_gdf("Metro Manila")

# Data set cleaning

# Filter listings to only show those within Metro Manila
mm_listings = gpd.sjoin(tripadvisor_gdf, mm_boundary[["geometry"]])
# Remove created "index_right" column and reset index count
mm_listings = mm_listings.drop("index_right", axis=1).reset_index(drop=True)

#mm_listings.head()

# Transfrom mm_listing column values into the appropriate format

# Convert baseDailyRate into float
mm_listings["baseDailyRate"] = mm_listings["baseDailyRate"].str["amount"].astype(float)

# Per row, sum all values of bedroomInfo then convert into integer
mm_listings["bedroomInfo"] = mm_listings["bedroomInfo"].str.findall(r'\b\d+\b').apply(lambda x: sum(map(int, x)))

#mm_listings.head()

# Amenities selected as explanatory variables: "gym" and "air conditioning"
# Check if said amenities exist in a listing
def amenity_exists(amenities, amenity_name):
  return 1 if amenity_name in amenities else 0

 # Search for amenities
mm_listings["gym"] = mm_listings["amenities"].apply(lambda x: amenity_exists(x, "Gym"))
mm_listings["air_conditioning"] = mm_listings["amenities"].apply(lambda x: amenity_exists(x, "Air conditioning"))
mm_listings["air_conditioning"] = mm_listings["amenities"].apply(lambda x: 1 if x == 1 else amenity_exists(x, "Central heating"))


#mm_listings.head()

# Load Road Network of Metro Manila
road = ox.features_from_place("Metro Manila", tags={"highway":["primary"]})

# Load Restaurant data of Metro Manila
resto = ox.features_from_place("Metro Manila", tags={"amenity":["restaurant"]})

# Filter data to convert Restaurant data into points and Road Network into Lines
resto_pts = ox.project_gdf(resto)[resto.geometry.type == "Point"]
road_lines = ox.project_gdf(road)[road.geometry.type == "LineString"]

road_lines.explore()
resto_pts.explore()

# Fit all features into the same CRS then join (WGS 84 / UTM Zone 51N)
mm_listings = mm_listings.to_crs(32651)
road_lines = road_lines.to_crs(32651)
resto_pts = resto_pts.to_crs(32651)

# Calculate shortest distance between listing and nearest restaurant
nearest_resto = gpd.sjoin_nearest(mm_listings, resto_pts, distance_col="distance_to_restaurant").dropna(axis=1).reset_index(drop=True)
nearest_resto = nearest_resto[["name_left","distance_to_restaurant"]].rename(columns={"name_left":"name"})
#nearest_resto.head()

# Calculate shortest distance between listing and nearest primary road
nearest_road = gpd.sjoin_nearest(mm_listings, road_lines, distance_col="distance_to_road").dropna(axis=1).reset_index(drop=True)
nearest_road = nearest_road[["name_left","distance_to_road"]].rename(columns={"name_left":"name"})
#nearest_road.head()

# Merge distances to road lines and restaurants with the Metro Manila GeoDataFrame
mm_listings = mm_listings.merge(nearest_resto, on="name").merge(nearest_road, on="name")

# Clean data
mm_listings = mm_listings.drop_duplicates(subset="name")
mm_listings = mm_listings.dropna().reset_index(drop=True)
print(mm_listings.head())

# Create Map
mm_listings.explore(
    column = "baseDailyRate",
    cmap = "YlGn",
    scheme = "Quantiles",
    k = 4,
    tooltip = ["name", "baseDailyRate", "bathCount", "air_conditioning", "gym", "bedroomInfo", "distance_to_restaurant", "distance_to_road"],
    vmax = 500,
    tiles = "Cartodb voyager"
)

"""## Problem 2 - Make a correlation matrix (1 point)

In this problem, we start our statistical analysis by investigating the correlation between the price and the variables we have selected. One of the assumptions of Ordinary Least Squares regression is that there should be linear relationship between variables. Hence, before doing anything else, it is good to check whether this assumption You should:

- Calculate a correlation matrix between all the variables in our model
   - Use the trip data GeoDataFrame as the source data for the correlation matrix and store the results in a new DataFrame called `correlation_matrix`.
   - Round the values so that they have only 2 decimals.
   - Hint: Check the pandas documentation for `corr()` method (see [docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)).
- Visualize the correlation matrix as a heatmap using Seaborn's [heatmap()](https://seaborn.pydata.org/generated/seaborn.heatmap.html) functionality. You should annotate the correlation values so that they are visible in the plot, and answer to the questions (scroll down to find them).

<br>

Please write your solution to the cell below (remove the `raise NotImplementedError()` code). You can create new cells as well if needed.
"""

# Import needed modules
import pandas as gd
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize values for correlation analysis
values_col = ["baseDailyRate", "bathCount", "bedroomInfo", "air_conditioning", "gym", "distance_to_restaurant", "distance_to_road"]
correlation_matrix = mm_listings[values_col]

# Calculate correlation matrix
correlation_matrix = round(correlation_matrix.corr(method="pearson"), 2)
#correlation_matrix

# Visualize correlation matrix
plt.figure(figsize=(10,10))

sns.heatmap(correlation_matrix,
            annot=True,
            vmin = -1,
            vmax = 1,
            center = 0)

plt.title("Heatmap of Metro Manila TripAdvisor Listings Variables")
plt.show()

"""### Questions

- **Question 2.1.** Which three (3) variables have the strongest correlation with price, and what are their correlation coefficients?
- **Question 2.2.** Which variable you might want to drop due to low correlation with price? (Note: do not drop the variable from your data, only answer to the question based on your understanding)
- **Question 2.3.** Multicollinearity should be avoided in OLS, meaning that there shouldn't be a relationship between the explanatory variables. One way to detect multicollinearity is to investigate whether there are high correlation values between the explanatory variables (a typical "rule of thumb" cutoff value is 0.8, although lower thresholds are used as well). Based on the correlation matrix, do you see issues with multicollinearity in our variables? Justify your answer with a sentence or two.

### Answers

Answer to questions above by adding text after the `Answer` bullet points below:

(*Hint*: double-click this cell to activate editing)

- **Answer for Q2.1**:

The three (3) variables with the strongest positive correlation with price or baseDailyRate are bathCount (0.15), gym (0.06), and bedroominfo (0.01). It should be noted that, while still positively correlated, gym and bedroominfo are weakly correlated to price. in the case of bedroominfo, this could be due to the lack of specified bedroom numbers in the listings dataset.


The two (2) variables with the strongest negative correlation with price or baseDailyRate are the distance_to_road (-0.03) and distance_to_resto (-0.04). Note that air_conditioning was not considered due to having no correlation (0) with price.

- **Answer for Q2.2**:

The best variable to drop would be air_conditioning. It has no correlation (0) with the listing price, thus could be removed or replaced with more fitting explanatory variables.
- **Answer for Q2.3**:


Given that the "rule of thumb" cutoff value for detecting multicollinearity is 0.8, there are no issues with multicollinearity in the explanatory variables used. The highest correlation value in the matrix is 0.15 (bathCount) and is less than the given cutoff value.

## Problem 3 - Is there spatial autocorrelation in the price values? (2 points)

As we learned during the lesson this week, spatial autocorrelation is something that can influence quite a bit how well our statistical models work. Hence, it is good to try to understand if our dependent variable (*price*) have spatial autocorrelation. It is always good to investigate more analytically (using here `Moran's I` indicator), whether the values have spatial correlation or not. To do this, you should:

- Create spatial weights based on how the vacation rental points are far from each other. Create the weights based on the data GeoDataFrame using **Distance Band** and store the resulting weights into a variable `w`. For creating the spatial weights, you can use the `weights` submodule from `pysal` library (see [docs](https://pysal.org/libpysal/generated/libpysal.weights.DistanceBand.html)). Use a threshold of 2 km for generating the weights.

- Calculate the `Moran's I` based on the "price" attribute and using the spatial weights that we created in the previous step. For doing this, you can use the [Moran()](https://pysal.org/esda/generated/esda.Moran.html) function from the pysal library, which accepts the Series of our price column as one parameter and the weights as another, check the pysal docs for details. What is the global Moran's I for our data?

- Create a Moran plot based on our data that allows us to investigate the spatial autocorrelation visually. For doing this, you can use a [plot_moran()](https://splot.readthedocs.io/en/latest/generated/splot.esda.plot_moran.html) -function from pysal's splot submodule.

<br>

Please write your solution to the cell below (remove the `raise NotImplementedError()` code). You can create new cells as well if needed.
"""

# Import needed modules
from pysal.lib import weights
from esda.moran import Moran
from splot.esda import plot_moran

# Create spatial weights based on listing distances from each other
# Threshold for Distance Band: 2 km or 2000 m
w = weights.DistanceBand.from_dataframe(mm_listings, threshold=2000)

# Calculate Moran's I
moran = Moran(mm_listings["baseDailyRate"], w)
print(moran)

# Plot Moran's I
plot_moran(moran, zstandard=True)
plt.show()

"""## Problem 4 - Spatial regression (3 points)


As an overview, in this problem, we will:

1. Do log transformations to some of our variables (to ensure the relationship between dependent and explanatory variable is ~ linear)

2. conduct three different regression models:

   - A normal Ordinary Least Squares (OLS) regression model
   - An OLS model with proximity variables, taking into account Spatial Heterogeneity.
   - Spatial lag model (i.e. considering the price of the neighboring areas also as an explanatory variable)

### Task 4.1 - Ordinary Least Squares (OLS)

Here we do our first regression model using Ordinary Least Squares regression (use the GeoDataFrame as input data):

1. Perform logarithmic transformation on the price and store it as "log_price" column
2. Create OLS where the "log_price" is the dependent variable and number of bathrooms, number of beds, and amenity-related variables as the explanatory variables
3. Print out the summary of the regression model and answer to questions (scroll down for questions)

<br>

Please write your solution to the cell below (remove the `raise NotImplementedError()` code). You can create new cells as well if needed.
"""

# Import needed modules
import pysal as ps
from pysal.model import spreg
from scipy import stats

# Tripadvisor GeoDataFrame as the input data
# Copy mm_listings into a another df for analysis
OLS_input_data = mm_listings.copy()

# Logarithmic transformation of price values
OLS_input_data["log_price"] = np.log(OLS_input_data["baseDailyRate"] + 1e-6)

# OLS regression
# Define amenities chosen as explanatory variables
# Integrate weights used in Moran's I prior
ex_variables = ["bathCount", "bedroomInfo", "gym", "air_conditioning"]
OLS_model = spreg.OLS(OLS_input_data["log_price"].values,
                      OLS_input_data[ex_variables].values,
                      w=w,
                      name_y="log_price",
                      name_x=ex_variables,
                      spat_diag=True,
                      moran=True
                      )
print(OLS_model.summary)

"""### Questions

- **Question 4.1.** What is the R-squared value of our model?
- **Question 4.2.** Which of the explanatory variables has the highest coefficient?

### Answers

Answer to questions above by adding text after the `Answer` bullet points below:

(*Hint*: double-click this cell to activate editing)

- **Answer for Q4.1**: 0.2902
- **Answer for Q4.2**: bathCount (0.54263)

### Task 4.2: Spatial Heterogeneity

In this task, we will learn how introducing spatial effects by adding proximity variables influences our regression model (use the GeoDataFrame as input data). You should:

- Use the same weights which were done in problem 3
- Add the two proximity variables (distance to restaurant, distance to nearest highway) in our regression model
- Do a regular OLS regression in a similar manner as in Step 1 (having "log_price" as the dependent variable), but now include these spatially proximity variables as explanatory variables. Print the summary of the model and answer to questions (scroll down to find them):

<br>

Please write your solution to the cell below (remove the `raise NotImplementedError()` code). You can create new cells as well if needed.
"""

# Add proximity variables to explanatory variables used in OLS
spatial_vars = ex_variables + ["distance_to_restaurant", "distance_to_road"]

# Copy mm_listings into a another df for spatial analysis
spatial_input_data = mm_listings.copy()

# Repeat Logarithmic transformation of price values
spatial_input_data["log_price"] = np.log(spatial_input_data["baseDailyRate"] + 1e-6)

# Repeat OLS regression model using spatial_vars
# Integrate weights used in Moran's I prior
spatial_heterogeneity_model = spreg.OLS(spatial_input_data["log_price"].values,
                      spatial_input_data[spatial_vars].values,
                      w=w,
                      name_y="log_price",
                      name_x=spatial_vars,
                      spat_diag=True,
                      moran=True
                      )
print(spatial_heterogeneity_model.summary)

"""### Questions

- **Question 4.3.** What is the R-squared value of our model? Did it improve?
- **Question 4.4.** Which of the explanatory variables has the highest coefficient?

### Answers

Answer to questions above by adding text after the `Answer` bullet points below:

(*Hint*: double-click this cell to activate editing)

- **Answer for Q4.3**:  0.2998

R-squared value improved from 0.2902 to 0.2998 after integrating spatial proximity variables
- **Answer for Q4.4**: bathCount (0.54773)

### Task 4.3: Spatial lag model


In this task, we will learn how introducing spatial effects also into the dependent variables influence our regression model, i.e. we will do a spatial lag model (use the GeoDataFrame as input data). You should:

1. Use the same weights which were used in previous step
2. Conduct a spatial two stage least squares regression (i.e. a spatial lag model) in a similar manner as shown in the tutorial/tech session. This time, you should use the same explanatory variables as in our first OLS model, i.e. number of beds, number of baths, amenity variables. Print the summary of the model and answer to questions (scroll down to find them).

<br>

Please write your solution to the cell below (remove the `raise NotImplementedError()` code). You can create new cells as well if needed.
"""

# Copy mm_listings into a another df for spatial analysis
spatial_lag_data = mm_listings.copy()

# Repeat Logarithmic transformation of price values
spatial_lag_data["log_price"] = np.log(spatial_lag_data["baseDailyRate"] + 1e-6)

# Spatial Lag Model
spatial_lag_model = spreg.GM_Lag(spatial_lag_data["log_price"].values,
                      spatial_lag_data[spatial_vars].values,
                      w=w,
                      name_y="log_price",
                      name_x=spatial_vars,
                      spat_diag=True
                      )
print(spatial_lag_model.summary)

"""### Questions

- **Question 4.5.** What is the Pseudo R-squared value of our model?
- **Question 4.6.** Which of the explanatory variables has the highest coefficient? Does it make sense to you? Explain with a sentence or two.

### Answers

Answer to questions above by adding text after the `Answer` bullet points below:

(*Hint*: double-click this cell to activate editing)

- **Answer for Q4.5**: 0.3128
- **Answer for Q4.6**: bathCount (0.56484)

This makes sense. Many listings only include shared bathrooms or one private bathroom per room. The inclusion of multiple private bathrooms per room would drive up the price among renters, especially large groups.

# Part 2 - How long did it take? Optional feedback (1 point)

To help developing the exercises, and understanding the time that it took for you to finish the Exercise, please provide an estimate of how many hours you spent for doing this exercise? *__Hint:__ To "activate" this cell in Editing mode, double click this cell. If you want to get this cell back in the "Reading-mode", press Shift+Enter.*

I spent approximately this many hours: **24 hours**

In addition, if you would like to give any feedback about the exercise (optional), please provide it below:

**My feedback:**

Honestly, the reason it took me around 24 hours to complete the laboratory exercise was mainly due to external factors unrelated to academic factors. I was handling family matters which staggered my progress with the lab exerise.

I did encounter some trouble with the unfamiliarity of the needed scripts and modules. I had to look up the documentation for those and ask for assistance to other people in the class on how to use them. This slowed down my progress but not by much. Furthermore, the scripts for Problems 3 and 4 were similar enough that only a few tweaks are required.
"""